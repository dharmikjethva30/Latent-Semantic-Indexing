{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rArD4tj4QA6t",
        "outputId": "3d595054-0b1a-4c54-ab3a-d50a0913fd87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download stopwords and initialize stemmer\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0# Text preprocessing\n",
        "def preprocess_text(text):\n",
        "    tokens = text.split()\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "preprocessed_data = [preprocess_text(text) for text in newsgroups_data.data]\n"
      ],
      "metadata": {
        "id": "_nuzWZnPQLEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_data)\n"
      ],
      "metadata": {
        "id": "bGd-RfcIRHbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply SVD (LSI)\n",
        "num_topics = 100\n",
        "lsa = TruncatedSVD(n_components=num_topics)\n",
        "lsa_matrix = lsa.fit_transform(tfidf_matrix)\n"
      ],
      "metadata": {
        "id": "MYctkR5hRPZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top terms for each topic\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "for i in range(num_topics):\n",
        "    top_terms_idx = lsa.components_[i].argsort()[-10:][::-1]\n",
        "    top_terms = [terms[idx] for idx in top_terms_idx]\n",
        "    print(f\"Topic {i + 1}: {', '.join(top_terms)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPU77GV8RStS",
        "outputId": "1767dfe5-d184-4013-a7ed-fdc0bf4f2220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: use, like, know, think, peopl, time, make, say, good, work\n",
            "Topic 2: window, use, file, card, drive, thank, pleas, program, mail, anyon\n",
            "Topic 3: game, team, year, play, player, car, hockey, season, win, fan\n",
            "Topic 4: pleas, game, thank, mail, anyon, god, know, thanks, edu, post\n",
            "Topic 5: drive, god, scsi, card, disk, hard, ide, christian, car, floppi\n",
            "Topic 6: window, game, god, file, run, team, win, play, program, problem\n",
            "Topic 7: key, god, game, chip, encrypt, clipper, edu, 00, use, phone\n",
            "Topic 8: know, key, anyon, chip, game, encrypt, thank, clipper, think, like\n",
            "Topic 9: drive, file, scsi, disk, game, peopl, israel, armenian, post, hard\n",
            "Topic 10: card, video, driver, monitor, armenian, israel, peopl, vga, color, arab\n",
            "Topic 11: edu, geb, pitt, n3jxp, dsl, cadre, chastiti, intellect, gordon, skeptic\n",
            "Topic 12: window, car, key, god, chip, drive, edu, pleas, run, right\n",
            "Topic 13: know, anyon, file, armenian, car, god, format, year, program, convert\n",
            "Topic 14: file, card, think, driver, car, look, key, ftp, format, bit\n",
            "Topic 15: use, car, file, game, gun, pleas, thank, right, card, driver\n",
            "Topic 16: problem, time, run, post, com, work, year, thank, tri, mail\n",
            "Topic 17: car, anyon, think, driver, christian, read, year, book, team, window\n",
            "Topic 18: know, gun, anyon, com, 00, fbi, koresh, batf, file, law\n",
            "Topic 19: thank, gun, god, space, advance, edu, look, program, good, team\n",
            "Topic 20: thank, 00, think, problem, advance, file, right, work, sale, make\n",
            "Topic 21: israel, edu, arab, isra, com, right, god, run, know, jew\n",
            "Topic 22: game, space, car, program, problem, need, run, law, thanks, control\n",
            "Topic 23: game, edu, armenian, think, thank, peopl, car, com, mac, program\n",
            "Topic 24: think, armenian, mail, peopl, work, year, want, need, good, modem\n",
            "Topic 25: good, bike, book, com, christian, need, read, want, church, run\n",
            "Topic 26: bike, space, know, card, use, want, game, ride, driver, 00\n",
            "Topic 27: like, new, problem, post, year, edu, driver, israel, sound, thank\n",
            "Topic 28: anyon, think, bike, look, drive, game, thanks, com, problem, new\n",
            "Topic 29: com, like, say, mail, list, thank, look, port, modem, 00\n",
            "Topic 30: monitor, know, color, edu, drive, christian, want, key, imag, display\n",
            "Topic 31: say, work, driver, pleas, thanks, problem, 00, tri, make, scsi\n",
            "Topic 32: look, good, program, want, fbi, say, book, 00, think, israel\n",
            "Topic 33: program, mail, problem, list, 00, peopl, run, key, christian, like\n",
            "Topic 34: post, right, god, read, color, peopl, make, want, imag, printer\n",
            "Topic 35: say, program, monitor, anyon, good, card, disk, post, drive, 00\n",
            "Topic 36: need, work, peopl, want, thanks, christian, right, post, church, day\n",
            "Topic 37: say, scsi, new, mac, list, want, mail, look, year, printer\n",
            "Topic 38: look, good, file, edu, problem, want, mail, 00, list, run\n",
            "Topic 39: right, book, good, anyon, read, time, scsi, want, pleas, problem\n",
            "Topic 40: need, question, team, book, new, know, driver, like, want, problem\n",
            "Topic 41: work, question, book, read, time, 00, number, like, phone, answer\n",
            "Topic 42: look, year, com, know, gun, problem, say, pleas, need, program\n",
            "Topic 43: right, question, version, driver, answer, 00, run, moral, year, look\n",
            "Topic 44: need, good, com, scsi, year, key, say, bit, printer, game\n",
            "Topic 45: question, want, scsi, com, good, new, thanks, answer, problem, program\n",
            "Topic 46: work, good, gun, key, modem, say, law, post, mail, program\n",
            "Topic 47: number, modem, anyon, program, port, question, want, time, serial, peopl\n",
            "Topic 48: need, anyon, key, post, want, time, offer, sale, look, space\n",
            "Topic 49: need, make, christian, program, 00, look, mail, edu, post, year\n",
            "Topic 50: time, new, make, look, program, long, work, need, pleas, team\n",
            "Topic 51: peopl, key, mac, read, look, book, team, monitor, disk, 00\n",
            "Topic 52: driver, time, question, run, monitor, printer, answer, christian, say, chip\n",
            "Topic 53: make, question, sure, want, key, thanks, homosexu, answer, driver, mac\n",
            "Topic 54: gun, someon, new, want, monitor, time, believ, version, thanks, mac\n",
            "Topic 55: scsi, number, monitor, program, phone, homosexu, someon, post, problem, list\n",
            "Topic 56: list, peopl, chip, number, need, right, gun, problem, program, sound\n",
            "Topic 57: new, tri, scsi, monitor, need, homosexu, program, number, printer, run\n",
            "Topic 58: want, work, mean, disk, need, anyon, year, read, someth, code\n",
            "Topic 59: make, need, pleas, new, peopl, anyon, year, problem, question, version\n",
            "Topic 60: power, monitor, peopl, list, control, program, scsi, help, book, key\n",
            "Topic 61: 00, thing, mean, printer, chip, someon, font, list, port, edu\n",
            "Topic 62: book, someon, sure, 10, help, tri, believ, board, list, need\n",
            "Topic 63: tri, law, sure, come, version, disk, ftp, sin, send, way\n",
            "Topic 64: board, help, bit, 00, stuff, day, gun, said, team, realli\n",
            "Topic 65: homosexu, board, say, time, good, gay, believ, power, server, men\n",
            "Topic 66: chip, list, someon, help, test, right, gun, run, send, believ\n",
            "Topic 67: font, someon, disk, card, way, print, email, christian, address, sure\n",
            "Topic 68: mac, test, law, day, got, list, disk, stuff, number, font\n",
            "Topic 69: pleas, tri, price, read, group, buy, cost, list, test, right\n",
            "Topic 70: sure, list, mean, power, email, bit, read, address, program, number\n",
            "Topic 71: law, list, format, imag, tell, someon, disk, homosexu, convert, gif\n",
            "Topic 72: thing, said, read, day, mail, someth, version, code, group, color\n",
            "Topic 73: mean, mac, got, read, ll, power, sound, comput, come, believ\n",
            "Topic 74: disk, heard, help, play, modem, player, realli, believ, cd, point\n",
            "Topic 75: stuff, heard, deleted, font, group, tri, thing, read, pleas, sound\n",
            "Topic 76: way, homosexu, test, got, sure, gay, stuff, best, men, sex\n",
            "Topic 77: stuff, lot, modem, tri, mean, version, deleted, thanks, program, video\n",
            "Topic 78: got, heard, sure, pc, mean, email, talk, board, anybodi, list\n",
            "Topic 79: mean, disk, version, help, number, word, chip, ll, come, said\n",
            "Topic 80: thing, price, better, test, start, board, list, line, differ, church\n",
            "Topic 81: sound, color, got, church, day, buy, heard, board, team, test\n",
            "Topic 82: test, said, stuff, pc, sound, board, thanks, deleted, color, mr\n",
            "Topic 83: thing, cd, help, info, way, mac, number, believ, sell, data\n",
            "Topic 84: come, test, let, way, day, data, monitor, heard, talk, board\n",
            "Topic 85: tell, someon, better, mail, pc, speed, number, convert, sound, version\n",
            "Topic 86: bit, heard, monitor, day, data, tri, someon, book, encrypt, buy\n",
            "Topic 87: team, got, speed, mean, tell, price, win, cd, data, hi\n",
            "Topic 88: tell, info, ask, moral, read, help, stuff, sure, ll, yes\n",
            "Topic 89: believ, someth, way, let, ll, state, church, cd, driver, format\n",
            "Topic 90: point, buy, email, ll, test, old, david, power, faq, realli\n",
            "Topic 91: got, version, line, hi, group, msg, cd, law, believ, food\n",
            "Topic 92: group, let, church, got, fan, mani, sure, someon, state, test\n",
            "Topic 93: got, articl, number, power, true, david, sure, font, port, object\n",
            "Topic 94: ask, differ, test, let, color, group, someon, power, year, tri\n",
            "Topic 95: line, inform, repli, point, port, support, ll, heard, format, someon\n",
            "Topic 96: buy, play, info, line, start, mani, test, someth, ask, format\n",
            "Topic 97: someth, let, sound, talk, address, better, info, email, comput, read\n",
            "Topic 98: phone, come, someth, articl, buy, true, differ, group, best, sure\n",
            "Topic 99: better, ask, hi, anybodi, pc, church, server, case, price, test\n",
            "Topic 100: line, start, believ, islam, buy, david, realli, day, board, data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample query\n",
        "query =\"formula\"\n",
        "\n",
        "# Preprocess the query\n",
        "query = preprocess_text(query)\n",
        "\n",
        "# Project the query into the LSI space\n",
        "query_vector = tfidf_vectorizer.transform([query])\n",
        "query_lsa = lsa.transform(query_vector)\n",
        "\n",
        "# Compute cosine similarity between the query and documents\n",
        "similarities = cosine_similarity(query_lsa, lsa_matrix)\n",
        "most_similar_doc_idx = similarities.argmax()\n",
        "most_similar_doc = newsgroups_data.data[most_similar_doc_idx]\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(\"Most similar document:\")\n",
        "print(most_similar_doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il37s0LsRVaw",
        "outputId": "4b053f9a-688b-4402-c255-904499182fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: formula\n",
            "Most similar document:\n",
            "Disclaimer -- This is for fun.\n",
            "\n",
            "In my computerized baseball game, I keep track of a category called\n",
            "\"stolen hits\", defined as a play made that \"an average fielder would not\n",
            "make with average effort.\"  Using the 1992 Defensive Averages posted\n",
            "by Sherri Nichols (Thanks Sherri!), I've figured out some defensive stats\n",
            "for the leftfielders. Hits Stolen have been redefined as \"Plays Kevin\n",
            "Bass would not have made.\"\n",
            "\n",
            "OK, I realize that's unfair.  Kevin's probably the victim of pitching staff,\n",
            "fluke shots, and a monster park factor.  But let's put it this way:  If we\n",
            "replaced every leftfielder in the league with someone with Kevin's 49.4% out\n",
            "making ability, how many extra hits would go by?\n",
            "\n",
            "To try and correlate it to reality a little more, I've calculated Net\n",
            "Hits Stolen, based on the number of outs made compared to what a league\n",
            "average fielder would make.  By the same method I've calculated Net Extra \n",
            "Bases (doubles and triples let by).\n",
            "\n",
            "Finally, I throw all this into a a formula I call Defensive Contribution, or\n",
            "DCON :->.  Basically, it represents the defensive contribution of a player.\n",
            "I add this number to OPS to get DOPS (Defense + Onbase Plus Slug), which\n",
            "should represent the player's total contribution to the team.  So don't\n",
            "take it too seriously.  The formula for DCON appears at the end of this\n",
            "article.\n",
            "\n",
            "The short version -- definition of terms\n",
            "HS -- Hits Stolen -- Extra outs compared to Kurt Stillwell\n",
            "NHS -- Net Hits Stolen -- Extra outs compared to average fielder\n",
            "NDP -- Net Double Plays -- Extra double plays turned compared to avg fielder\n",
            "NEB -- Net Extra Bases --  Extra bases prevented compared to avg. fielder\n",
            "DCON -- Defensive Contribution -- bases and hits prevented, as a rate.\n",
            "DOPS -- DCON + OPS -- quick & dirty measure of player's total contribution.\n",
            "\n",
            "National League\n",
            "\n",
            "Name            HS   NHS   NEB   DCON    DOPS\n",
            "Gonzalez, L.    63    28    20   .192    .866\n",
            "Gilkey, B.      52    23    14   .150    .941\n",
            "Clark, G.       46    11    11   .065    .726\n",
            "Alou, M.        20     3    12   .052    .835\n",
            "Bonds, B.       54     9    -7   .019   1.099\n",
            "May, D.         21     0    -7  -.020    .659\n",
            "Gant, R.        31    -5    -2  -.021    .715\n",
            "Bass, K.         0   -24    -4  -.126    .600\n",
            "\n",
            "Ordered by DOPS\n",
            "\n",
            "1.099 Bonds\n",
            " .941 Gilkey\n",
            " .866 Gonzalez\n",
            " .835 Alou\n",
            " .726 Clark\n",
            " .718 *NL Average*\n",
            " .715 Gant\n",
            " .659 May\n",
            " .600 Bass\n",
            "\n",
            "\n",
            "American League\n",
            "---------------\n",
            "\n",
            "Name            HS   NHS   NEB   DCON    DOPS\n",
            "Raines, T.      53    22    20   .111    .896\n",
            "Anderson, B.    65    30     8   .102    .924\n",
            "Henderson, R.   43    20     4   .101    .984\n",
            "Vaughn, G.      55    27    -3   .095    .817\n",
            "Gladden, D.     25     4     8   .038    .699\n",
            "Hall, M.        29     6    -2   .017    .756\n",
            "Mack, S.        38     6    -8   .005    .866\n",
            "Polonia, L.     10   -11    10  -.019    .647\n",
            "McReynolds, K.  13    -8    -9  -.064    .711\n",
            "Maldanado, C.    9   -21   -12  -.105    .714\n",
            "Reimer, K.       5   -18   -16  -.102    .671\n",
            "\n",
            "\n",
            "Order by DOPS\n",
            "\n",
            ".984 Henderson\n",
            ".924 Anderson\n",
            ".896 Raines\n",
            ".866 Mack\n",
            ".817 Vaughn\n",
            ".756 Hall\n",
            ".733 *AL Average*\n",
            ".714 Maldanado\n",
            ".711 McReynolds\n",
            ".699 Gladden\n",
            ".671 Reimer\n",
            ".647 Polonia\n",
            "\n",
            "More discussion --\n",
            "\n",
            "DCON formula:  ((NHS + NDP)/PA) + ((NHS + NDP + NEB)/AB)\n",
            "Why such a bizzare formula?  Basically, it's designed to be added into the\n",
            "OPS, with the idea that \"a run prevented is as important as a run scored\".\n",
            "The extra outs are factored into OBP, while the extra bases removed are \n",
            "factored into SLG.  That's why I used PA and AB as the divisors.\n",
            "\n",
            "For more discussion see the post on Hits Stolen -- First Base 1992\n",
            "-- \n",
            "Dale J. Stephenson |*| (steph@cs.uiuc.edu) |*| Baseball fanatic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "\n",
        "# Load the true labels (categories) for the dataset\n",
        "true_labels = newsgroups_data.target\n",
        "\n",
        "# Perform clustering on LSI-transformed data using KMeans\n",
        "kmeans = KMeans(n_clusters=20)  # Assuming 20 clusters for the 20 Newsgroups dataset\n",
        "lsi_cluster_labels = kmeans.fit_predict(lsa_matrix)\n",
        "\n",
        "# Evaluate the clustering quality\n",
        "ari_score = adjusted_rand_score(true_labels, lsi_cluster_labels)\n",
        "nmi_score = normalized_mutual_info_score(true_labels, lsi_cluster_labels)\n",
        "\n",
        "print(f\"Adjusted Rand Index (ARI): {ari_score}\")\n",
        "print(f\"Normalized Mutual Information (NMI): {nmi_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNd8D5vtUu9h",
        "outputId": "763db3a3-809b-49ba-e215-b96b433e0ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Rand Index (ARI): 0.06196552113425908\n",
            "Normalized Mutual Information (NMI): 0.2910276507226431\n"
          ]
        }
      ]
    }
  ]
}